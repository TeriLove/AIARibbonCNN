{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_k-fold.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EU2eYXohNR5",
        "colab_type": "text"
      },
      "source": [
        "## Ribbon CNN Training Notebook\n",
        "\n",
        "This notebook contains the code to train a simple CNN to classify different types of flare ribbons visible in 1600Ã… AIA (*Atmospheric Imaging Assembly*) images.\n",
        "\n",
        "To use this notebook the following packages are needed:\n",
        "\n",
        "(To train the network)\n",
        "1. numpy\n",
        "\n",
        "2. pickle (for reading in training data)\n",
        "\n",
        "3. keras\n",
        "\n",
        "4. scikit-learn\n",
        "\n",
        "(To create training/test plots)\n",
        "5. matplotlib\n",
        "\n",
        "6. pandas\n",
        "\n",
        "7. seaborn\n",
        "\n",
        "Note that with the training data included in *4class_data.pickle* a GPU is currently not needed to train this model, however if the amount of data is increased this would have to change. \n",
        "\n",
        "The parameters chosen (epochs, batch size etc) are selected to optimize the network performance that corresponds to the training set, again if the training set is altered these may also have to be modified.\n",
        "\n",
        "Note that in this notebook k-fold cross-validation has also been implemented (where k = 5), this is to ensure a more vigorus training of the model with varying validation sets used throughout training. For more information on cross validation please see [here](https://machinelearningmastery.com/k-fold-cross-validation/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jj7Xu2Wq5Bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training packages\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras import optimizers\n",
        "#plotting packages\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import interp\n",
        "from itertools import cycle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq75hST_m9qE",
        "colab_type": "text"
      },
      "source": [
        "We will initially read in the training and test data, with the model parameters also defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj6miZGGrQqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import data\n",
        "with open('4class_data_model3.pickle','rb') as t:\n",
        "\t input_train, target_train, input_test, target_test = pickle.load(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6p8G28rso1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model configuration\n",
        "batch_size = 32\n",
        "img_width, img_height, img_num_channels = 250, 250, 1\n",
        "no_classes = 4\n",
        "no_epochs = 10\n",
        "validation_split = 0.4\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "input_shape = (img_width, img_height, img_num_channels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzc5yVF-ryGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define per-fold acc/loss lists\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "histories =[]\n",
        "model_history = []\n",
        "\n",
        "#Define training set\n",
        "inputs = input_train\n",
        "targets = target_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZvVGGA_ns8q",
        "colab_type": "text"
      },
      "source": [
        "K-fold cross-validation implemented below (the model code can be extracted from here is cross-validation is too computationally expensive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPko-ETrsDw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):  \n",
        "# Define the model architecture  \n",
        "\tmodel = Sequential()  \n",
        "\tmodel.add(Convolution2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))  \n",
        "\tmodel.add(Convolution2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Dense(128, activation='relu'))\n",
        "\tmodel.add(Dense(4, activation='softmax'))\n",
        "\tsgd = sgd_op(lr=0.001, clipval=0.5)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\t# Fit data to model\n",
        "\thistory = model.fit(inputs[train], targets[train], batch_size=batch_size, epochs=no_epochs, verbose=verbosity, validation_split=validation_split)\n",
        "\thistories.append(history)\n",
        "  model_history.append(model)\n",
        "\t# Generate generalization metrics\n",
        "\tscores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "\tprint(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "\tacc_per_fold.append(scores[1] * 100)\n",
        "\tloss_per_fold.append(scores[0])\n",
        "\t# Increase fold number\n",
        "\tfold_no = fold_no + 1\n",
        "\t# == Provide average scores ==\n",
        "\tprint('------------------------------')\n",
        "\tprint('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "\tprint('------------------------------')\n",
        "\tprint(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "\tprint('------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('-------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx3ZdXp4oA7r",
        "colab_type": "text"
      },
      "source": [
        "The model is now sufficently trained - the plotting routines listed below are just some examples of how the model can be tested and results plotted.\n",
        "\n",
        "First, the results from the cross-validation are plotted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqc_fOo6pMXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " for i in range(len(histories)):\n",
        "     # plot loss\n",
        "     plt.subplot(211)\n",
        "     plt.title('Cross Entropy Loss')\n",
        "     plt.plot(range(1, 11), histories[i].history['loss'], color='blue', label='train')\n",
        "     plt.plot(range(1, 11), histories[i].history['val_loss'],color='orange', label='validation')\n",
        "     plt.xlabel('Epoch')\n",
        "     plt.ylabel('Loss')\n",
        "     if i==0:\n",
        "         plt.legend()\n",
        "     # plot accuracy\n",
        "     plt.subplot(212)\n",
        "     plt.title('Classification Accuracy')\n",
        "     plt.plot(range(1, 11), histories[i].history['acc'], color='blue', label='train')\n",
        "     plt.plot(range(1, 11), histories[i].history['val_acc'], color='orange', label='validation')\n",
        "     plt.xlabel('Epoch')\n",
        "     plt.ylabel('Accuracy')\n",
        "     if i ==0:\n",
        "         plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oLyjJyVufPm",
        "colab_type": "text"
      },
      "source": [
        "A confusion matrix using the test data set is created below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1PfopfJt69j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test trained model.\n",
        "testout = model.predict(input_test)\n",
        "\n",
        "matrix = sklearn.metrics.confusion_matrix(testout.argmax(axis=1), np.array(target_test))\n",
        "normmatrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "classes = ['background', '2 ribbon', 'limb', 'compact']\n",
        "df_cm = pd.DataFrame(normmatrix, index = classes,columns = classes)\n",
        "ax= plt.subplot()\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "ax.set_ylim(len(matrix), -0.5)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('True Class')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}